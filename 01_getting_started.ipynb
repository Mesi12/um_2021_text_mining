{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset/processed/stories/1_the_adventures_of_sherlock_holmes/a_scandal_in_bohemia.txt\") as fp:\n",
    "    data = fp.read()\n",
    "\n",
    "# clean processed input further\n",
    "# todo add these steps to our preprocessing step\n",
    "data = data.replace(\"\\n\", \" \")\n",
    "data = re.sub(r\" +\", \" \", data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo recognize quotes as direct speech\n",
    "# todo sherlock holmes is split into two tokens\n",
    "sent = nltk.tokenize.sent_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was not that he felt any emotion akin to love for Irene Adler.\n",
      "['It', 'was', 'not', 'that', 'he', 'felt', 'any', 'emotion', 'akin', 'to', 'love', 'for', 'Irene', 'Adler', '.']\n"
     ]
    }
   ],
   "source": [
    "sentence_id = 3\n",
    "words = nltk.tokenize.word_tokenize(sent[sentence_id])\n",
    "\n",
    "#print([word for word in words if word not in string.punctuation])\n",
    "print(sent[sentence_id])\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pos tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.help.upenn_tagset()\n",
    "tagged = nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('It', 'PRP'),\n",
       " ('was', 'VBD'),\n",
       " ('not', 'RB'),\n",
       " ('that', 'IN'),\n",
       " ('he', 'PRP'),\n",
       " ('felt', 'VBD'),\n",
       " ('any', 'DT'),\n",
       " ('emotion', 'NN'),\n",
       " ('akin', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('love', 'VB'),\n",
       " ('for', 'IN'),\n",
       " ('Irene', 'NNP'),\n",
       " ('Adler', 'NNP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## named entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  It/PRP\n",
      "  was/VBD\n",
      "  not/RB\n",
      "  that/IN\n",
      "  he/PRP\n",
      "  felt/VBD\n",
      "  any/DT\n",
      "  emotion/NN\n",
      "  akin/NN\n",
      "  to/TO\n",
      "  love/VB\n",
      "  for/IN\n",
      "  (PERSON Irene/NNP Adler/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "entities = nltk.chunk.ne_chunk(tagged)\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It',\n",
       " 'was',\n",
       " 'not',\n",
       " 'that',\n",
       " 'he',\n",
       " 'felt',\n",
       " 'any',\n",
       " 'emotion',\n",
       " 'akin',\n",
       " 'to',\n",
       " 'love',\n",
       " 'for',\n",
       " Tree('PERSON', ['Irene', 'Adler']),\n",
       " '.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/44237087/ne-chunk-without-pos-tag-in-nltk\n",
    "simple = []\n",
    "for elt in entities:\n",
    "    if isinstance(elt, nltk.Tree):\n",
    "        simple.append(nltk.Tree(elt.label(), [ word for word, tag in elt ]))\n",
    "    else:\n",
    "        simple.append( elt[0] )\n",
    "        \n",
    "simple        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51676 total words contain 6232 unique words.\n"
     ]
    }
   ],
   "source": [
    "# todo stemming\n",
    "# todo lemmatization\n",
    "\n",
    "# rudimentary version\n",
    "wordlist = nltk.word_tokenize(data)\n",
    "unique_words = set(wordlist)\n",
    "\n",
    "print(f\"{len(wordlist)} total words contain {len(unique_words)} unique words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slightly better\n",
    "# https://programminghistorian.org/en/lessons/counting-frequencies\n",
    "\n",
    "wordfreq = [wordlist.count(p) for p in wordlist]\n",
    "freqdict = dict(list(zip(wordlist,wordfreq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = [(freqdict[key], key) for key in freqdict]\n",
    "aux.sort()\n",
    "aux.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2959, ','),\n",
       " (2406, '.'),\n",
       " (2327, 'the'),\n",
       " (1322, 'and'),\n",
       " (1204, 'of'),\n",
       " (1076, 'to'),\n",
       " (963, 'a'),\n",
       " (940, 'I'),\n",
       " (886, '``'),\n",
       " (811, \"''\"),\n",
       " (674, 'in'),\n",
       " (649, 'was'),\n",
       " (630, 'he'),\n",
       " (619, 'that'),\n",
       " (613, 'his'),\n",
       " (471, 'had'),\n",
       " (454, 'it'),\n",
       " (369, 'you'),\n",
       " (315, 'which'),\n",
       " (313, 'with'),\n",
       " (303, 'for'),\n",
       " (300, 'as'),\n",
       " (291, 'is'),\n",
       " (289, 'at'),\n",
       " (278, 'have'),\n",
       " (274, 'my'),\n",
       " (271, 'him'),\n",
       " (249, 'be'),\n",
       " (212, 'me'),\n",
       " (208, 'The'),\n",
       " (208, '?'),\n",
       " (207, 'said'),\n",
       " (195, 'upon'),\n",
       " (193, \"'s\"),\n",
       " (192, 'on'),\n",
       " (179, 'this'),\n",
       " (179, 'not'),\n",
       " (175, 'from'),\n",
       " (173, 'but'),\n",
       " (173, 'He'),\n",
       " (172, 'all'),\n",
       " (169, 'were'),\n",
       " (164, 'her'),\n",
       " (160, 'there'),\n",
       " (155, 'man'),\n",
       " (151, 'by'),\n",
       " (149, 'one'),\n",
       " (147, 'been'),\n",
       " (142, 'we'),\n",
       " (142, 'them'),\n",
       " (142, 'no'),\n",
       " (139, 'so'),\n",
       " (139, 'It'),\n",
       " (135, \"'\"),\n",
       " (133, 'are'),\n",
       " (132, 'they'),\n",
       " (130, 'up'),\n",
       " (128, 'an'),\n",
       " (125, '--'),\n",
       " (119, 'out'),\n",
       " (118, 'would'),\n",
       " (115, 'or'),\n",
       " (112, 'who'),\n",
       " (107, 'do'),\n",
       " (104, ';'),\n",
       " (101, 'some'),\n",
       " (100, 'when'),\n",
       " (99, 'into'),\n",
       " (99, 'You'),\n",
       " (98, 'could'),\n",
       " (98, 'Holmes'),\n",
       " (97, 'There'),\n",
       " (94, 'down'),\n",
       " (91, 'will'),\n",
       " (91, 'their'),\n",
       " (88, \"n't\"),\n",
       " (85, 'over'),\n",
       " (85, '!'),\n",
       " (84, 'your'),\n",
       " (82, 'more'),\n",
       " (80, 'what'),\n",
       " (80, 'little'),\n",
       " (79, 'has'),\n",
       " (76, 'very'),\n",
       " (76, 'time'),\n",
       " (76, 'about'),\n",
       " (75, 'two'),\n",
       " (75, 'she'),\n",
       " (74, 'before'),\n",
       " (73, 'other'),\n",
       " (70, 'any'),\n",
       " (69, 'way'),\n",
       " (67, 'if'),\n",
       " (67, 'face'),\n",
       " (67, 'came'),\n",
       " (65, 'asked'),\n",
       " (64, 'come'),\n",
       " (62, 'Drebber'),\n",
       " (61, 'then'),\n",
       " (61, 'Ferrier')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# frequent words / tokens\n",
    "# todo need cleaning\n",
    "# todo need stopword removal\n",
    "# todo need merging of words: stemming, lemmatization...\n",
    "aux[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
