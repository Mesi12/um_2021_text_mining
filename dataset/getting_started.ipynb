{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process file: processed\\novels\\a_study_in_scarlet.txt\n"
     ]
    }
   ],
   "source": [
    "PATH_INPUT = \"processed\"\n",
    "PATH_NOVELS = \"novels\"\n",
    "FOLDER_PATH = os.path.join(PATH_INPUT,PATH_NOVELS)\n",
    "f = \"a_study_in_scarlet.txt\"\n",
    "filepath = os.path.join(FOLDER_PATH,f)\n",
    "\n",
    "if os.path.isfile(filepath) and f.endswith(\".txt\"):\n",
    "    print(f\"Process file: {filepath}\")\n",
    "\n",
    "    with open(filepath, \"r\") as fp:\n",
    "        content = fp.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'part'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-111-20d78766e981>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mfrequency\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcontent_lower\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mfrequency\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfrequency\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 'part'"
     ]
    }
   ],
   "source": [
    "#regexp to get all words separately without punctuation\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "content_removed_punct = tokenizer.tokenize(content)\n",
    "\n",
    "#make words lowercase\n",
    "content_lower = []\n",
    "for word in content_removed_punct:\n",
    "    content_lower.append(word.lower())\n",
    "#print(content_lower)  \n",
    "\n",
    "#count frequency of words with lists\n",
    "words = []\n",
    "frequencies = []\n",
    "for word in content_lower:\n",
    "    if word not in words:\n",
    "        words.append(word)\n",
    "        frequencies\n",
    "    else:\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count frequency of words with dictionary\n",
    "frequency = {}\n",
    "frequency[w] = 0\n",
    "for word in content_lower:\n",
    "    frequency[word] = frequency[word] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['                                      PART I                   (Being a reprint from the reminiscences of                              John H. Watson, M.D.,                      lat']\n"
     ]
    }
   ],
   "source": [
    "#tokenize content by sentences and remove newlines\n",
    "content_sent = sent_tokenize(content)\n",
    "a = content_sent[:200]\n",
    "#a.str.replace('\\n',\"\")\n",
    "#a = list(map(lambda s: s.strip(), a))\n",
    "#a = list(filter(lambda item: item not in ['\\n', '\\t'], a))\n",
    "#print(a)\n",
    "new_content = []\n",
    "for sub in a:\n",
    "    new_content.append(re.sub('\\n', '', sub))\n",
    "print(new_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sentence length: 181.0\n"
     ]
    }
   ],
   "source": [
    "summ = 0\n",
    "n = 0\n",
    "for sentence in new_content:\n",
    "    n += 1\n",
    "    summ += len(sentence)\n",
    "avg = summ/n\n",
    "print(f\"Average sentence length: {avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"He is swimming. She dances. When have they done cooking? Who is careful enough to not hit the bottles? I'm on it!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He is swimming.',\n",
       " 'She dances.',\n",
       " 'When did they cook?',\n",
       " 'Who is careful enough to not hit the bottles?',\n",
       " \"I'm on it!\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He\n",
      "is\n",
      "swimming\n",
      ".\n",
      "She\n",
      "dance\n",
      ".\n",
      "When\n",
      "have\n",
      "they\n",
      "done\n",
      "cooking\n",
      "?\n",
      "Who\n",
      "is\n",
      "careful\n",
      "enough\n",
      "to\n",
      "not\n",
      "hit\n",
      "the\n",
      "bottle\n",
      "?\n",
      "I\n",
      "'m\n",
      "on\n",
      "it\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(text)\n",
    "lemmatizer = WordNetLemmatizer\n",
    "for w in words:\n",
    "    lemma = lemmatizer().lemmatize(w)\n",
    "    print(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
