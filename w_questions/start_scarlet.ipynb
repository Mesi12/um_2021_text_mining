{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Process file: dataset\\processed\\novels\\a_study_in_scarlet.txt\n"
     ]
    }
   ],
   "source": [
    "PATH_DATASET = \"dataset\"\n",
    "PATH_INPUT = \"processed\"\n",
    "PATH_NOVELS = \"novels\"\n",
    "FOLDER_PATH = os.path.join(PATH_DATASET,PATH_INPUT,PATH_NOVELS)\n",
    "f = \"a_study_in_scarlet.txt\"\n",
    "filepath = os.path.join(FOLDER_PATH,f)\n",
    "\n",
    "if os.path.isfile(filepath) and f.endswith(\".txt\"):\n",
    "    print(f\"Process file: {filepath}\")\n",
    "\n",
    "    with open(filepath, \"r\") as fp:\n",
    "        content = fp.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean content from empty lines, excessive spaces, and chapter title to have only the actual text\n",
    "clean_content = re.sub('\\n',' ',content)\n",
    "clean_content = re.sub(' +',' ',clean_content)\n",
    "\n",
    "#todo find this for every chapter with regex\n",
    "#todo whats up with -- (what are these for?)\n",
    "clean_content = clean_content.replace(' PART I (Being a reprint from the reminiscences of John H. Watson, M.D., late of the Army Medical Department.) CHAPTER I ','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization\n",
    "content_sent = nltk.tokenize.sent_tokenize(clean_content)\n",
    "content_word = nltk.tokenize.word_tokenize(clean_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pos tagging\n",
    "pos_content = nltk.pos_tag(content_word[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(',', 2953), ('the', 2522), ('.', 2399), ('and', 1354), ('of', 1207), ('to', 1083), ('a', 996), ('i', 937), ('``', 885), (\"''\", 810), ('he', 802), ('in', 723), ('that', 673), ('was', 652), ('his', 651), ('it', 591), ('had', 479), ('you', 466), ('with', 335), ('as', 332), ('at', 319), ('which', 316), ('for', 312), ('my', 306), ('is', 298), ('have', 290), ('him', 272), ('there', 257), ('be', 249), ('on', 227), ('this', 217), ('me', 212), ('?', 208), ('said', 207), ('upon', 198), (\"'s\", 192), ('we', 192), ('but', 187), ('not', 184), ('all', 184), ('from', 182), ('they', 180), ('no', 173), ('her', 173), ('were', 169), ('one', 162), ('by', 159), ('so', 159), ('man', 155), ('been', 147), ('them', 142), ('when', 140), ('are', 136), (\"'\", 135), ('up', 130), ('an', 128), ('do', 124), ('what', 123), ('would', 122), ('--', 120), ('out', 119), ('who', 116), ('or', 113), (';', 104), ('some', 103), ('if', 103), ('could', 99), ('into', 99), ('holmes', 97), ('down', 95), ('will', 94), ('their', 93), ('she', 91), (\"n't\", 88), ('!', 85), ('over', 85), ('your', 85), ('then', 84), ('more', 83), ('now', 83), ('little', 82), ('before', 80), ('very', 80), ('has', 80), ('two', 79), ('time', 76), ('about', 76), ('other', 73), ('come', 70), ('any', 69), ('way', 69), ('came', 67), ('face', 67), ('our', 65), ('asked', 65), ('how', 63), ('drebber', 62), ('see', 61), ('ferrier', 61), ('through', 60)]\n"
     ]
    }
   ],
   "source": [
    "#preproc for word count\n",
    "content_lower = []\n",
    "for word in content_word:\n",
    "    content_lower.append(word.lower())\n",
    "    \n",
    "#count frequency of words and print 100 most common\n",
    "frequency = {}\n",
    "for word in content_lower:\n",
    "    frequency[word] = frequency.get(word,0) + 1\n",
    "    \n",
    "#sort dictionary according to value (descending order)  \n",
    "sorted_freq = sorted(frequency.items(), key = lambda x: x[1], reverse = True)\n",
    "\n",
    "#print 100 most common words\n",
    "print(sorted_freq[:100])\n",
    "\n",
    "#ignore punctuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Average sentence length: 87.30126300148588\n"
     ]
    }
   ],
   "source": [
    "#count average sentence length\n",
    "summ = 0\n",
    "n = 0\n",
    "for sentence in content_sent:\n",
    "    n += 1\n",
    "    summ += len(sentence)\n",
    "avg = summ/n\n",
    "print(f\"Average sentence length: {avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python371jvsc74a57bd07f92eed155a7eff10eae3d43382e03fbf37478cd9a086bf360d0c4fa35627eb7",
   "display_name": "Python 3.7.1 64-bit (conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}