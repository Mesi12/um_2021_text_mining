{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process file: dataset\\processed\\novels\\a_study_in_scarlet.txt\n"
     ]
    }
   ],
   "source": [
    "PATH_DATASET = \"dataset\"\n",
    "PATH_INPUT = \"processed\"\n",
    "PATH_NOVELS = \"novels\"\n",
    "FOLDER_PATH = os.path.join(PATH_DATASET,PATH_INPUT,PATH_NOVELS)\n",
    "f = \"a_study_in_scarlet.txt\"\n",
    "filepath = os.path.join(FOLDER_PATH,f)\n",
    "\n",
    "if os.path.isfile(filepath) and f.endswith(\".txt\"):\n",
    "    print(f\"Process file: {filepath}\")\n",
    "\n",
    "    with open(filepath, \"r\") as fp:\n",
    "        content = fp.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean content from empty lines, excessive spaces, and chapter title to have only the actual text\n",
    "\n",
    "clean_content = re.sub('\\n',' ',content)\n",
    "clean_content =clean_content.replace(' PART I (Being a reprint from the reminiscences of John H. Watson, M.D., late of the Army Medical Department.) CHAPTER I ','')\n",
    "clean_content = re.sub(' +',' ',clean_content)\n",
    "#print(content[:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize text by sentences\n",
    "content_sent = sent_tokenize(clean_content)\n",
    "\n",
    "#remove puntiation for word tokenization\n",
    "#cleaner_content = re.sub('[.,;!?\"\"]','',clean_content)\n",
    "content_word = word_tokenize(clean_content)\n",
    "#print(content_word)\n",
    "#remove chapter 2 and such\n",
    "#whats up with -- ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pos tagging\n",
    "pos_content = pos_tag(content_word[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regexp to get all words separately without punctuation\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "content_removed_punct = tokenizer.tokenize(content)\n",
    "\n",
    "#make words lowercase\n",
    "content_lower = []\n",
    "for word in content_removed_punct:\n",
    "    content_lower.append(word.lower())\n",
    "#print(content_lower)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 2526), ('and', 1356), ('of', 1210), ('to', 1092), ('a', 1004), ('i', 940), ('he', 804), ('in', 725), ('that', 673), ('his', 652), ('was', 651), ('it', 595), ('had', 476), ('you', 470), ('with', 335), ('as', 332), ('at', 321), ('which', 316), ('for', 313), ('my', 307), ('is', 296), ('have', 290), ('him', 272), ('there', 258), ('be', 249), ('on', 231), ('this', 217), ('me', 213), ('said', 207), ('upon', 198), ('s', 193), ('we', 193), ('but', 190), ('all', 186), ('from', 183), ('they', 180), ('not', 177), ('no', 176), ('her', 173), ('were', 169), ('one', 163), ('by', 160), ('so', 160), ('man', 155), ('been', 147), ('them', 142), ('when', 141), ('are', 136), ('up', 133), ('an', 129), ('what', 125), ('would', 123), ('out', 120), ('who', 118), ('or', 113), ('some', 103), ('if', 103), ('into', 99), ('holmes', 97), ('could', 97), ('down', 95), ('will', 95), ('their', 93), ('she', 91), ('t', 88), ('over', 86), ('your', 86), ('do', 86), ('now', 85), ('more', 84), ('then', 84), ('little', 82), ('before', 80), ('very', 80), ('has', 80), ('two', 79), ('time', 77), ('about', 77), ('other', 73), ('come', 70), ('way', 70), ('any', 69), ('well', 68), ('came', 67), ('face', 67), ('our', 65), ('asked', 65), ('how', 63), ('drebber', 62), ('see', 61), ('ferrier', 61), ('through', 60), ('answered', 59), ('eyes', 59), ('than', 57), ('room', 57), ('hand', 57), ('hope', 56), ('found', 55), ('should', 55)]\n"
     ]
    }
   ],
   "source": [
    "#count frequency of words and print 100 most common\n",
    "frequency = {}\n",
    "for word in content_lower:\n",
    "    frequency[word] = frequency.get(word,0) + 1\n",
    "    \n",
    "#sort dictionary according to value (descending order)  \n",
    "sorted_freq = sorted(frequency.items(), key = lambda x: x[1], reverse = True)\n",
    "\n",
    "#print 100 most common words\n",
    "print(sorted_freq[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize content by sentences and remove newlines\n",
    "content = content.replace('     ','').replace('\\n','')\n",
    "content_sent = sent_tokenize(content)\n",
    "\n",
    "new_content = []\n",
    "for sub in content_sent:\n",
    "    new_content.append(re.sub('\\n', '', sub))\n",
    "#print(new_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_word = word_tokenize(content)\n",
    "#print(content_word[:1000])\n",
    "#print(pos_tag(content_word[:300]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sentence length: 86.19420720386186\n"
     ]
    }
   ],
   "source": [
    "#count average sentence length\n",
    "summ = 0\n",
    "n = 0\n",
    "for sentence in new_content:\n",
    "    n += 1\n",
    "    summ += len(sentence)\n",
    "avg = summ/n\n",
    "print(f\"Average sentence length: {avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
